# Importacion de librearias
import streamlit as st
import joblib
import pandas as pd
import json
import logging
import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns
import csv
from sklearn.preprocessing import StandardScaler, LabelEncoder
import os
from cryptography.fernet import Fernet


#Configuraci√≥n de login para auditorias
logging.basicConfig(filename="audit_log.txt", level=logging.INFO, format="%(asctime)s - %(message)s")

# Cargar los roles del json
with open("config.json", "r") as f:
    roles = json.load(f)

key = Fernet.generate_key()
cipher_suite = Fernet(key)


class eda:
     def __init__(self):
          self.df = None
          self.clave = None
          self.cipher = None
          self._load_or_generate_key()

     def _load_or_generate_key(self):
        """Carga la clave de encriptaci√≥n desde un archivo o genera una nueva."""
        key_path = "encryption_key.key"
        if os.path.exists(key_path):
            with open(key_path, "rb") as file:
                self.clave = file.read()
        else:
            self.clave = Fernet.generate_key()
            with open(key_path, "wb") as file:
                file.write(self.clave)
        self.cipher = Fernet(self.clave)    

     def encrypt_column(self, column_name):
        """
        Encripta una columna del dataset si existe.
        """
        self._check_df()
        if column_name not in self.df.columns:
            raise KeyError(f"La columna '{column_name}' no existe en el dataset.")

        self.df[column_name + "_Encriptado"] = self.df[column_name].apply(
            lambda x: self.cipher.encrypt(str(x).encode()).decode()
        )
        logging.info(f"Columna '{column_name}' encriptada exitosamente.")

     def save_encrypted_data(self, column_name, filename="data_encriptada.csv"):
        """Guarda el dataset con los datos encriptados y elimina la columna original."""
        self._check_df()
        self.encrypt_column(column_name)

        self.df.to_csv(filename, index=False) #Guardar en csv
        logging.info(f"Datos encriptados guardados en {filename}.")

        if column_name in self.df.columns:
            self.df.drop(columns=[column_name], inplace=True)
            logging.info(f"Columna original '{column_name}' eliminada del DataFrame.")

        if column_name + "_Encriptado" in self.df.columns:
            self.df.drop(columns=[column_name + "_Encriptado"], inplace=True)
            logging.info(f"Columna encriptada '{column_name}_Encriptado' eliminada del DataFrame.")

     def _check_df(self):
        """Verifica si el DataFrame est√° cargado."""
        if self.df is None:
            logging.warning("El DataFrame no ha sido cargado. Usa 'read_dataset()' primero.")
            raise ValueError("El DataFrame est√° vac√≠o.")
        
     def read_dataset(self, path):
            """
            Cargar el dataset e identificar el separador.
            """
            separa = None
            try:
                if path.startswith("http"):
                    self.df = pd.read_csv(path, sep=";", decimal=".")
                else:
                    with open(path, 'r', encoding='utf-8') as file:
                        sample = file.read(800)  # Leer una muestra del archivo
                        st.write("Muestra del archivo:", sample)
                        if not sample.strip():
                            raise ValueError("El archivo est√° vac√≠o o no contiene suficientes datos para detectar el separador.")

                        dialect = csv.Sniffer().sniff(sample)  
                        separa = dialect.delimiter
                        st.write("Separador detectado:", separa)

                    self.df = pd.read_csv(path, sep=separa, decimal=".",header=0, index_col=None)
            
                logging.info("Datos cargados con √©xito. Separador detectado: '%s'" % separa)

            except FileNotFoundError:
                logging.error(f"No se encontr√≥ el archivo en la ruta: {path}")
                raise FileNotFoundError(f"No se encontr√≥ el archivo en la ruta: {path}")

            except UnicodeDecodeError:
                logging.error("Error de codificaci√≥n: intenta usar una codificaci√≥n distinta como 'latin1'.")
                raise UnicodeDecodeError("Error de codificaci√≥n. Verifica el archivo y la codificaci√≥n.")   
            except Exception as e:
                logging.error(f"Error al cargar el archivo: {path}: {str(e)}")
                raise

     def show_head(self, n):
        """Muestra las primeras filas del DataFrame."""
        self._check_df()
        st.markdown(f"<u> Estos son los primeros {n} registros del dataset: </u>", unsafe_allow_html=True)
        return self.df.head(n)

     def check_missing_values(self):
        """Revisa los valores faltantes en el DataFrame."""
        self._check_df()
        missing_values = self.df.isnull().sum()
        st.markdown("<u> Valores faltantes por columna: </u>", unsafe_allow_html=True)
        if missing_values.sum() == 0:
            st.write("¬°Todos los valores est√°n completos! No hay valores faltantes.")
        else:
            st.write(missing_values)

     def identify_duplicates(self):
        """Identifica duplicados en el DataFrame."""
        self._check_df()
        duplicated = self.df.duplicated()
        total_duplicates = duplicated.sum()

        st.markdown("<u> Valores duplicados: </u>", unsafe_allow_html=True)
        if total_duplicates > 0:
            st.write(f"Se encontraron {total_duplicates} registros duplicados.")
            st.write("Extracto de los datos duplicados:")
            st.write(self.df[duplicated].head())
            return True # hay duplicados
        else:
            st.write("No se encontraron registros duplicados en el dataset.")
            return False # no hay duplicados

     def analisisNumerico(self):
        """
        Verificar si hay columnas num√©ricas, mostrar un conteo y el nombre de las columnas.
        """
        self._check_df() 
        st.markdown("<u> Columna(s) num√©rica(s) </u>", unsafe_allow_html=True)
        try:
            numeric_columns = self.df.select_dtypes(include=["number"]).columns

            if numeric_columns.size > 0:
                st.write(f"El dataset contiene {len(numeric_columns)} columna(s) num√©rica(s).")
                st.write("Estas son las columnas num√©ricas:")
                st.write(list(numeric_columns))
            else:
                st.warning("El dataset no contiene columnas num√©ricas.")


            return list(numeric_columns)

        except Exception as e:
            st.error(f"Ocurri√≥ un error al analizar las columnas num√©ricas: {e}")
            logging.error(f"Error en analisisNumerico: {e}")
            return []

     def check_data_types(self):
        """Muestra los tipos de datos de cada columna del DataFrame."""
        self._check_df()
        st.markdown("<u> Tipos de datos por columna: </u>", unsafe_allow_html=True)
        return self.df.dtypes

# --------------------  Data cleanning  -------------------- #

     def drop_duplicates(self):
        """Elimina duplicados del DataFrame."""
        self._check_df()
        before = len(self.df)
        self.df = self.df.drop_duplicates()
        after = len(self.df)
        st.markdown("<u> Registros duplicados: </u> ", unsafe_allow_html=True)
        st.write(f"Duplicados eliminados. Se pas√≥ de {before} a {after} registros.")
 
     def _transform_column(self, column_name, transform_type, all_columns=False):
        """
        Transforma una columna en variables dummy o valores categ√≥ricos codificados.
        Si `all_columns` es True, aplica la transformaci√≥n a todas las columnas categ√≥ricas.
        """
        try:
            if all_columns:
                categorical_cols = self.df.select_dtypes(include=["object", "category"]).columns
                categorical_cols = [col for col in categorical_cols if col != "Pais Destino"]  # Excluir la variable objetivo
    
                for col in categorical_cols:
                    if transform_type == "Variables dummy":
                        dummies = pd.get_dummies(self.df[col], prefix=col)
                        self.df = pd.concat([self.df.drop(columns=[col]), dummies], axis=1)
                    elif transform_type == "C√≥digos categ√≥ricos":
                        self.df[col] = pd.Categorical(self.df[col]).codes
                st.success(f"Todas las columnas categ√≥ricas fueron transformadas a '{transform_type}'.")
    
            else:
                # Transformar una columna espec√≠fica
                if transform_type == "Variables dummy":
                    dummies = pd.get_dummies(self.df[column_name], prefix=column_name)
                    self.df = pd.concat([self.df.drop(columns=[column_name]), dummies], axis=1)
                elif transform_type == "C√≥digos categ√≥ricos":
                    self.df[column_name] = pd.Categorical(self.df[column_name]).codes
                st.success(f"La columna '{column_name}' fue transformada usando '{transform_type}'.")
    
            # **Convertir TODAS las columnas num√©ricas a enteros**
            for col in self.df.select_dtypes(include=['float64', 'float32']).columns:
                self.df[col] = self.df[col].round(0).astype(int)
    
            # **Asegurar que "Pais Destino" se convierte en c√≥digos enteros**
            if "Pais Destino" in self.df.columns:
                self.df["Pais Destino"] = pd.Categorical(self.df["Pais Destino"]).codes.astype(int)
                st.success("‚úî La columna 'Pais Destino' ha sido transformada correctamente en c√≥digos enteros.")
    
        except Exception as e:
            st.warning(f"No se pudo transformar la columna '{column_name}' o las columnas categ√≥ricas: {e}")

     def type_transform(self):
        """
        Permite al usuario seleccionar c√≥mo transformar las columnas categ√≥ricas: 
        variables dummy o c√≥digos categ√≥ricos. Aplica la transformaci√≥n a una columna espec√≠fica
        o a todas las columnas categ√≥ricas.
        """
        self._check_df()
        try:
            st.markdown("<u>Transformar columnas categ√≥ricas</u>", unsafe_allow_html=True)
            transform_options = ["Variables dummy", "C√≥digos categ√≥ricos"]
            
            # Escoge entre transformar una columna o todas las columnas
            option = st.radio("¬øDeseas transformar una columna espec√≠fica o todas las categ√≥ricas?", 
                            ("Columna espec√≠fica", "Todas las columnas categ√≥ricas"))

            # Selecciona el tipo de transformaci√≥n (dummy o categ√≥rico)
            selected_transform = st.selectbox("Selecciona el tipo de transformaci√≥n:", transform_options)

            if option == "Columna espec√≠fica":
                column_name = st.selectbox("Selecciona la columna a transformar:", 
                                        self.df.select_dtypes(include=["object", "category"]).columns)
                if st.button("Transformar columna"):
                    self._transform_column(column_name, selected_transform)

            elif option == "Todas las columnas categ√≥ricas":
                if st.button("Transformar todas las columnas"):
                    self._transform_column(None, selected_transform, all_columns=True)

        except Exception as e:
            st.error(f"Error al intentar transformar las columnas categ√≥ricas: {e}")

     def describe_data(self):
        """Genera estad√≠sticas descriptivas b√°sicas del DataFrame."""
        self._check_df()
        st.markdown("<u> Estad√≠stica descriptiva del conjunto de datos: </u>", unsafe_allow_html=True)
        return self.df.describe()            

     def delete_irrelevant_values(self):
        """
        Elimina columnas del DataFrame que tienen un √∫nico valor √∫nico (sin variabilidad).
        """
        self._check_df()  
        try:
            columnas_constantes = [col for col in self.df.columns if self.df[col].nunique() <= 1]
            
            if columnas_constantes:
                st.markdown("<u> Datos constantes que ser√°n eliminados. </u>", unsafe_allow_html=True)
                st.write(f"Se encontraron {len(columnas_constantes)} columna(s) constante(s) que ser√°n eliminadas:")
                st.write(columnas_constantes)
                
                self.df = self.df.drop(columns=columnas_constantes)

                st.success("Columnas constantes eliminadas exitosamente.")
            else:
                st.write("No se encontraron columnas constantes en el DataFrame.")

        except Exception as e:
            st.error(f"Un error ocurri√≥ mientras se eliminaban las columnas constantes: {e}")
            logging.error(f"Error al eliminar columnas constantes: {e}")

     def plot_distributions(self):
        """Grafica la distribuci√≥n de las columnas num√©ricas."""
        self._check_df()
        numeric_cols = self.df.select_dtypes(include=["number"]).columns

        if numeric_cols.empty:
            st.write("No hay columnas num√©ricas para graficar.")
            return
        
        st.markdown("<u> Distribuciones de variables num√©ricas:</u>", unsafe_allow_html=True)
        num_cols_per_row = 4 # Agrupacion de los graficos
        col_chunks = [numeric_cols[i:i + num_cols_per_row] for i in range(0, len(numeric_cols), num_cols_per_row)]

        for chunk in col_chunks:
            cols = st.columns(len(chunk))  # Creamos columnas din√°micamente seg√∫n el tama√±o del grupo
            for col, column_name in zip(cols, chunk):
                with col:
                    fig, ax = plt.subplots()
                    sns.histplot(self.df[column_name], kde=True, ax=ax)
                    ax.set_title(f"Distribuci√≥n de {column_name}")
                    st.pyplot(fig)

     def correlation_matrix(self):
        """Muestra y grafica la matriz de correlaci√≥n."""
        self._check_df()
        corr = self.df.corr()
        st.markdown("<u> Matriz de correlaci√≥n: </u>", unsafe_allow_html=True)
        
        fig, ax = plt.subplots(figsize=(10, 6))
        sns.heatmap(corr, annot=True, cmap='coolwarm', fmt=".2f", ax=ax)
        ax.set_title("Mapa de calor de correlaci√≥n")
        st.pyplot(fig)

     def outlier_detection(self):
        """Detecta posibles valores at√≠picos usando diagramas de caja."""
        self._check_df()
        numeric_cols = self.df.select_dtypes(include=["number"]).columns

        if numeric_cols.empty:
            st.write("No hay columnas num√©ricas para graficar.")
            return
        
        st.markdown("<u> Diagramas de caja (Boxplots):</u>", unsafe_allow_html=True)
        
        num_cols_per_row = 3
        col_chunks = [numeric_cols[i:i + num_cols_per_row] for i in range(0, len(numeric_cols), num_cols_per_row)]

        for chunk in col_chunks:
            cols = st.columns(len(chunk))
            for col, column_name in zip(cols, chunk):
                with col:
                    fig, ax = plt.subplots()
                    sns.boxplot(x=self.df[column_name], ax=ax)
                    ax.set_title(f"Valores at√≠picos en {column_name}")
                    st.pyplot(fig)
     
     def advanced_outlier_detection(self):
        """Detecta valores at√≠picos utilizando el rango intercuartil (IQR)."""
        self._check_df()
        numeric_cols = self.df.select_dtypes(include=["number"]).columns

        st.markdown("<u> Detecci√≥n avanzada de valores at√≠picos:</u>", unsafe_allow_html=True)
        for col in numeric_cols:
            q1 = self.df[col].quantile(0.25)
            q3 = self.df[col].quantile(0.75)
            iqr = q3 - q1
            lower_bound = q1 - 1.5 * iqr
            upper_bound = q3 + 1.5 * iqr

            outliers = self.df[(self.df[col] < lower_bound) | (self.df[col] > upper_bound)]
            st.write(f"Valores at√≠picos en {col}: {len(outliers)} registros.")
            st.write(outliers)

     def standardize_data(self):
        """
        Estandariza las columnas num√©ricas del DataFrame.
        Transforma los datos para que tengan media 0 y desviaci√≥n est√°ndar 1.
        """
        self._check_df()
        numeric_cols = self.df.select_dtypes(include=["number"]).columns

        if numeric_cols.empty:
            st.warning("No hay columnas num√©ricas para estandarizar.")
            return

        try:
            scaler = StandardScaler()
            self.df[numeric_cols] = scaler.fit_transform(self.df[numeric_cols])
            st.success("Las columnas num√©ricas han sido estandarizadas con √©xito.")
            st.write(f"Columnas estandarizadas: {list(numeric_cols)}")
        except Exception as e:
            st.error(f"Error al estandarizar las columnas: {e}")


    # Funcion para verificiar permiso
     def check_access(role):
        """Esta funcion sirve para verificar los permisos de los roles """
        if role not in roles:
            logging.warning(f"Acceso denegado: Rol {role} no registrado")
            raise PermissionError("Acceso denegado: Rol no registrado")
        return roles[role]["access_level"]


class fileHandler:
        # Funcion para hacer backups
    def backup_data (df, filename="backup_data.csv"):

        df.to_csv(filename, index=False)
        logging.info(f"Backup realizado con exito en : {filename}")


    def save_final_data(df, filename="data_estandarizada.csv"):
        """Guarda los datos procesados en un archivo CSV."""
        df.to_csv(filename, index=False)
        logging.info(f"Datos finales estandarizados guardados con √©xito en {filename}")

    

class app:
    def main(self):
            st.markdown('<h3 class="custom-h3">An√°lisis de Datos de Fertilizantes üå±</h3>', unsafe_allow_html=True)
            st.write("")
            eda_instance = eda()
            backup_instance = fileHandler()

            #Extraccion desde GitHub
            path = "https://raw.githubusercontent.com/CSMore/Datasets/refs/heads/main/Fertilizantes_CR_2024.csv"

            try:
                eda_instance.read_dataset(path)
                eda_instance.save_encrypted_data("N√∫mero Documento", filename="datos_encriptados.csv")
                
                

                st.dataframe(eda_instance.show_head(6))
                st.write("")
                eda_instance.check_missing_values()
                st.write("")
                duplicados_existentes = eda_instance.identify_duplicates() #almacenar el resultado
                eda_instance.identify_duplicates()
                st.write("")
                eda_instance.analisisNumerico()
                st.write(eda_instance.check_data_types())

                fileHandler.backup_data(eda_instance.df, "backup_data.csv")
                st.write("***** Datos respaldados en 'Backup_data.csv'. *****")   

                st.write(" ********************************************************************* ")
                st.markdown('<h4 class="custom-h4">Limpieza de los datos üßπüìä</h4>', unsafe_allow_html=True)
                if duplicados_existentes:
                    eda_instance.drop_duplicates()
                st.write("")
                eda_instance.type_transform()
                st.write(eda_instance.check_data_types())
                eda_instance.delete_irrelevant_values()
                st.write(eda_instance.describe_data())
                eda_instance.plot_distributions()
                eda_instance.correlation_matrix()
                st.write("")
                eda_instance.outlier_detection()
                eda_instance.advanced_outlier_detection()
                eda_instance.standardize_data()

                fileHandler.save_final_data(eda_instance.df, "data_final_estandarizado.csv")
                st.success("Datos finales estandarizados guardados en 'data_final_estandarizado.csv'.")
                
                logging.info("Datos extra√≠dos con √©xito.")
            except Exception as e:
                st.error("Error al cargar los datos.")
                logging.error(f"Error al cargar los datos: {e}")
            
